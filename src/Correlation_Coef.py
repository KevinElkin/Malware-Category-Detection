import numpy as np
import pandas as pd
from scipy.sparse import lil_matrix, csr_matrix,save_npz,load_npz
from sklearn import svm
from scipy.sparse import hstack
from collections import defaultdict
import json 
from scipy import sparse
from scipy.sparse import rand
from scipy.sparse import csr_matrix
from scipy.sparse import coo_matrix, vstack
from tqdm import tqdm
import csv

def npzToCsv(filename, savename):
    np_Array=np.load(filename)
    arr=np_Array.files[0]
    np.savetxt("Matrix_Data/" + str(savename) + '.csv', np_Array[arr], delimiter=",")

    
    
def trainMetapath(Amatrix, OtherMatrix):
    metapath = OtherMatrix*Amatrix.T
    return metapath



def TrainModel(labelPath, metapath, Amatrix):
    df = pd.read_csv(labelPath, header=None)
    clf = svm.LinearSVC(C=0.1, multi_class = 'ovr', dual = False)
    ker = Amatrix*metapath
    clf.fit(ker, df[0])
    return clf



#get size of training data
def syntheticSize(BAtrans):
    return BAtrans.shape[0]



def createSyn(APIsize, appNum, dense):
    SynA = rand(int(APIsize), appNum, density=dense, format='csr')
    SynA.data[:] = 1
    return SynA.T



def makeTestA(Amatrix_test, SynA, path):
    testData = vstack([Amatrix_test,SynA])
    #write syn to json so we can use same syn and real apps accross all kernels
    save_npz(path, testData)
    return testData


def testKernel(testData, metapath, trained):
    testKer = testData*metapath
    preds = trained.predict(testKer)
    return preds
   
    
def getAPIs(path):
    with open(path) as json_file: 
        API_index = json.load(json_file) 

    API_index_swap = dict([(value, key) for key, value in API_index.items()]) 
    return API_index_swap


def corelateCoef(testData, APIdict, preds, outname):
    API_corelate = defaultdict(list)
    APIli = []
    predli = []
    denSynA = testData.todense()
    print("Getting app and API info....")
    for i in tqdm(range(testData.shape[1])):
        for j in range(len(denSynA)):
            API_corelate[APIdict[i]].append(denSynA.item(j, i))
    corelate = {}
    print("Getting correlation coef....")
    for api in tqdm(range(testData.shape[1])):
        var1 = API_corelate[APIdict[api]]
        corelate[APIdict[api]] = np.corrcoef(var1, preds)[0][1]
    with open(outname, 'w') as outfile:
        json.dump(corelate, outfile) 

    
def main():
    
    #Load in the data
    print("Loading in the Data...")
    Amatrix = load_npz('Matrix_Data/a_matrix_train.npz')
    Bmatrix = load_npz('Matrix_Data/b_matrix_train.npz')
    Pmatrix = load_npz('Matrix_Data/p_matrix_train.npz')
    Amatrix_realTest = load_npz('Matrix_Data/a_matrix_test.npz')
    
    #Create the metapaths
    
    print("Creating the metapath on training data...")
    BAmeta = trainMetapath(Amatrix, Bmatrix)
    #---->PAmeta = trainMetapath(Amatrix, Pmatrix)
    #---->Ameta = Amatrix.T
    
    #Use the same SynA and testData on all Kernels so once created dont run this line again for other kernels
    
    #--------------------------------------------------------------#
    print("Creating the Synth Apps...")
    SynA = createSyn(syntheticSize(BAmeta), 4000, 0.01)
    testData = makeTestA(Amatrix_realTest, SynA, 'Matrix_Data/A_Train_Syn_Real.npz') #<---Destination path for file and name
    #--------------------------------------------------------------#
    
    
    #IMPORTANT -> if you have already created the synthetic apps dont bother running the 2 lines of code above again. 
    #Instead just load in the file already created so we use the same test data of synthetic apps accross all kernels for 
    #consistency. Uncomment the lines of code below in the dashes to load in the testdata that was already created. 
    
    #----------------------------------------------------------------#
    #print("Loading in the Synth Apps...")
    #testData = load_npz('Matrix_Data/A_Train_Syn_Real.npz')
    #----------------------------------------------------------------#
    
    #Get the predictions for the test/synth apps
    
    print("Making the predicitions on the test data...")
    preds = testKernel(testData, BAmeta, TrainModel('Matrix_Data/training_labels.csv', BAmeta, Amatrix))
    #---->preds = testKernel(testData, PAmeta, TrainModel('Matrix_Data/training_labels.csv', PAmeta, Amatrix))
    #---->preds = testKernel(testData, Ameta, TrainModel('Matrix_Data/training_labels.csv', Ameta, Amatrix))
 
    #Get correlation coefs and save to JSON file. To filter these refer to Filter_Coef.py
    print("Calculating the Corr Coefs...")
    corelateCoef(testData, getAPIs('Matrix_Data/apis.json'), preds, 'ABA_Cor.json')
    #---->corelateCoef(testData, getAPIs('Matrix_Data/apis.json'), preds, 'APA_Cor.json')
    #---->corelateCoef(testData, getAPIs('Matrix_Data/apis.json'), preds, 'AA_Cor.json')
    
if __name__ == '__main__':
    main()